---
layout: post
title: AlexNet
categories: DL
description: AlexNet
keywords: NN, AlexNet
---

# AlexNet：

文章开头Introduction中介绍了，提高深度学习模型表现的方法有：更大的数据集，更强的模型，更好的防止过拟合技术。

ImageNet有超1.5千万张分属于22000类别的高分辨率图片，而因为AlexNet需要一个固定的输入维度，因此他们将所有图片下采样为256✖️256大小的固定分辨率。

## 结构：

### ReLU：

全称：Rectified Linear Units (ReLUs)，修正非线性单元

相比于之前常用的tanh，ReLU在深层卷积神经网络的训练中速度更快，下图是原文中给出的依据，是根据训练轮数来评判的。

### Local Response Normalization：

主要是针对局部的特性进行归一化

### Overlapping Pooling：

stride的大小小于池化操作窗口大小

## 减少过拟合：

### 数据增强：

平移、旋转图像，截取小块（类似于卷积）+翻转后的截取小块

改变RGB通道的强度（PCA）

### Dropout：

dropped out的神经元，不对前向传播有贡献，也不参与反向传播

### 总结：

没有用无监督的预训练方法，据说可以有助于提升模型效果



