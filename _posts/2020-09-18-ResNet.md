---
layout: post
title: ResNet
categories: Neural Network
description: ResNet
keywords: Neural Network, ResNet

---

# ResNet

ResNet <a href="https://arxiv.org/pdf/1512.03385.pdf ">原论文链接</a>，为了解决在传统网络的层数太深的时候，就会发生梯度爆炸、梯度消失以及退化情况，然后下面是原论文中给出的ResNet的两个主要内容：

1. 残差结构使得搭建超深层网络结构成为可能，能够解决退化问题；
2. 使用Batch Norm来加速训练，可以解决梯度消失和梯度爆炸的问题。

其中退化问题，可以简单理解成当网络越深时，准确度出现饱和，甚至出现下降，如下图所示，接下来，我将详细介绍一下残差结构和Batch Norm。

![image-20200724194650066](https://s2.loli.net/2021/12/18/KLhF7RDBuYsgTN6.png)

## 1. 残差结构：

### 1.1  残差块:

下图中的两个残差块结构，左边结构主要是使用在ResNet-18 和 ResNet-34，右边的结构主要是使用在更深层的ResNet-101和ResNet-152。

![image-20200724194327470](https://s2.loli.net/2021/12/18/OgV7w81ujTL4EYN.png)

简单来看一下，这个模块的结构，就是分为直线（卷积+relu）和曲线，曲线这里就可以理解成对块输入啥都不操作，直接拿过来和块最后卷积的输出进行相加/concat，所以这里就要注意要保证concat的两个输入尺寸相同，也就是块中最后一层卷积的输出要和块的输入尺寸相同。

至于为啥深层的ResNet使用右边的这个残差块，我觉得是参数计算量要小很多，可以通过输入尺寸为（3，256，256）来计算一下，这里注意要保证上述提到过的条件外，同时还得保证最终两块feature map尺寸相同，所以就得把左侧层数调整为256，计算结果为：

左：3×3×256×256+3×3×256×256=1,179,648<br>右：1×1×256×64+3×3×64×64+1×1×64×256=69,632<br>

很明显，右侧运算次数要远少于左侧的。下图是ResNet-34的整体结构，但是可以发现，里面怎么还有曲线部分还有虚线的，下部分就来说一下，虚线和实线的区别：

![image-20200724200521083](https://s2.loli.net/2021/12/18/ibsk3HAp1qfFcZN.png)

### 1.2 Residual Block:

![ResNet1](https://s2.loli.net/2021/12/18/3I29mj8sfz7oSdM.png)

首先来看下，原论文中给出常用ResNet网络结构参数，可以看到output size那里，每一层feature map的size都缩小为之前的1/2，而我们通过计算可以知道，conv2_x那里的缩小是因为max pooling操作导致的，而下面的conv3_x、conv4_x、conv5_x，则是通过blocks中的第一个block来减小了feature map的size，同时扩大的channel数。也就是说，conv3_x、conv4_x、conv5_x中的blocks的第一层都是下两图中右半部分的结构。至于为啥stride=2，就缩小为1/2，根据out = ( in+2*padding-kernel_size )/stride+1计算，就可以得到。当然了这里仍然要保证输入输出的尺寸相同，因此虚线部分也要扩大channel。

![ResNet1](https://s2.loli.net/2021/12/18/jbnWGZpfCKTsi7D.png)

![image-20200724201909926](https://s2.loli.net/2021/12/18/vUmgDHGP6QpBN3W.png)

## 2.批量归一化：

Batch Norm的主要目的是让feature map满足方差为1、均值为0的分布，下图的蓝框是原论文中的描述

![](https://s2.loli.net/2021/12/18/xyz4oO2lrXc7k3S.png)

举个例子，来说下吧，首先假设有一个输入X是个RGB三通道的图像，所以说把输入图像通道 $d=3$，然后下面根据下图接着说。

![](https://s2.loli.net/2021/12/18/wu1jbPoRMKa3nsW.png)

首先计算一个batch中所有样本的方差$\sigma_B^2$和均值$\mu_B $，然后用每一个样本减去他们的均值$\mu_B $，最终除以平方差$\sqrt{\sigma_B^2+\epsilon}$，注意这里$\epsilon$是一个非常小的值，主要就是为了防止出现方差$\sigma_B^2$为0的情况。最终通过$\gamma$和$\beta$，来调整，它俩的初始化可以分别是1和0，然后再通过反向传播进行调整。

这里借用一下，一位大佬博客里的图（之后补上，引用链接），这是一个很清晰的例子，这里就不再赘述了。

![](https://s2.loli.net/2021/12/18/Elk25SFp3WqObcC.jpg)

最后说下，为啥现在的卷积网络dropout的使用逐渐在变少，首先BN和t一样拥有正则化功能，而dropout在卷积上的正则效果是有限的，此外卷积相对全连接层而言参数量更少，而且激活函数也可以完成特征的空间变换，而dropout表现很好的全连接层，它的作用正在被全局平均池化代替，因为后者不但可以减少模型大小还可以提高模型的表现。

<a href=" https://colab.research.google.com/drive/1TL4muG5BoNsNk5rd4u5KYqjEcsfTFc-m?usp=sharing">ResNet Colab</a>
