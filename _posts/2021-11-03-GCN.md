---
layout: post
title: Graph Convolutional Networks
description: GCN
keywords: Neural Networks
---

# Graph Convolutional Networks

## 1. 简单粗暴的理解一下原理：

本段内容，整理自B站up主小淡鸡的视频，链接如下：

GNN：https://www.bilibili.com/video/BV1Xy4y1i7sq?share_source=copy_web
GCN：https://www.bilibili.com/video/BV1Xy4y1i7sq?share_source=copy_web

### 1.1 Graph Neural Networks, GNN:

先来简单解释一下图神经网络，主要流程就是：**聚合、更新、循环**。

首先讲解下**聚合**，可以看到下图的图结构中一共包含A、B、C、D、E、F六个点，而点与点之间的连线就代表着这两点之间有某种联系，而每个点右下角的(X,X,X,X)用于表示该点的特征，这个特征可以是提取到的或者是标签，也可以是初始化的，这里其实把现在图上的特征说成**初始特征**，更利于后面内容的区分和理解。

![image-20210602190239441](/assets/img/GNN_1.png)

接下来，假设我们要对A进行分类，这就需要得到它的特征，而A的特征由两部分组成：自身+邻居信息。这里简单解释下为什么邻居信息会组成A的特征，介于上面提到的点与点之间有连线，说明这俩点之间有某种关系，所以说如果对A进行分类/定义，某种程度上而言，它的邻居B、C、D的特征也具有一定的参考价值，然后再用一句话来解释一下，就可以很明白的理解了，“近朱者赤，近墨者黑”。

最后可以把邻居信息N，用公式表示为，N = a\*(2,2,2,2) + b\*(3,3,3,3) + c\*(3,3,3,3)，其中的a、b、c大概就是超参数，可以进行优化、改进。

然后就到了**更新**的阶段，将更新后A的信息用公式表示为 A_new =  σ(W((1,1,1,1))+σ*N)，其中的σ表示激活函数，W( )为模型训练的参数。

然后再来说下GNN层数的一个点，让我们把目光重新放回到上面的那张图中和点A相连的点有B、C、D，也就是说A包含B、C、D的信息，所以我们就可以知道，而B包含A、C的信息，C包含A、B、E的信息，D包含A、F的信息。那么在第二层，也就是第二次聚合时，聚合A的信息，现在A已经包含了C的信息，而C中又包含了E的信息，所以A也就会获得E的特征，然后就可以得到每一个结点的最终特征，然后就可以进行结点分类或者关联预测了。

### 1.2 Graph Convolutional Networks, GCN:

图卷积神经网络，主要流程还是：聚合、更新、循环，而它与GNN之间的主要区别就在于**聚合**过程。首先，GCN就是为了解决GNN中邻居信息的公式里a、b、c值的设定，然后我们根据上面GNN的讲解来看，首先提出一个**平均法**，举个栗子就是：你认识的人工资的平均值=你的工资，然后可以你的工资用公式表示为：
$$
\begin{equation}
aggregate(X_i)=\sum_{j\in neighbor(i)}A_{ij}X_j
\end{equation}
$$

$$
aggregate(X)=AX
$$

但是上述公式忽略了自身的特征，针对于工资我们可以举例：学历、工作等，然后我们需要添加一个自环把自身特征加回来（单位矩阵），变成如下的公式：
$$
aggregate(X_i)=\sum_{j\in N}A_{ij}X_j+X_i
$$

$$
aggregate(X)&=(A+I)X\\
&=AX+X
$$

然后再求一个平均，引入度矩阵D，得到公式如下：
$$
\begin{aligned}
aggregate(X_i)&=\sum_{k=1}^{N} \hat{D_{ik}^{N}} \sum^{N}_{j=1} \hat{A_{ij}} X_j\\
&=\sum^{N}_{j=1}\hat{D_{ii}^{-1}}\hat{A_{ij}}X_j\\
&=\sum_{j=1}^{N}\frac{\hat{A_{ij}}}{\hat{D_{ii}}}X_j\\
&=\sum_{j=1}^{N}\frac{\hat{A_{ij}}}{\sum^N_{k=1}\hat{A_{ik}}}X_j
\end{aligned}
$$

$$
aggregate(X)=\hat{D^{-1}}\hat{A}X
$$

但是平均法有一个问题，我们来看下图，图中点A只与点B相连，而B与C、D、E等等相连，那么A的度就是1，带入到上述公式就是1/1的B的特征，所以说直接聚合B的特征来参考A，就有失偏颇了。

![image-20210602190239441](/assets/img/GCN_1.png) 

GCN为了解决上述这个问题，提出了如下的公式：
$$
H{(l+1)}=\sigma(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)}W^{(l)})
$$
该公式中的A邻接矩阵、D度矩阵，借用GNN中所用的图，表示如下图所示：

![image-20210602190239441](/assets/img/GNN_1.png)

![image-20210602190239441](/assets/img/GCN_2.png)
$$
\begin{aligned}
(\tilde{D}^{0.5}\hat{A}\tilde{D}^{0.5})_i&=(\tilde{D}^{0.5}\tilde{A})_i\tilde{D}^{0.5}H\\
&=\begin{pmatrix}\sum{\tilde{D}^{0.5}_{jk}}\tilde{A}_i\end{pmatrix} \tilde{D}^{0.5}H\\
&=\tilde{D}_{ii}^{0.5}\sum_j\tilde{A}_{ij}\sum_k\tilde{D}_{ij}^{0.5}H_j\\
&=\tilde{D}_{ii}{-0.5}\sum_j^i\tilde{A}_{ij}\tilde{A}_{jj}^{-0.5}H_j\\
&=\sum_j\frac{1}{\sqrt{\tilde{D}_{ii}}\tilde{D}_{jj}}\tilde{A}_{ij}H_j
\end{aligned}
$$
我们把上面的图-1，i当作点A，j当作点B，然后对应的D值分别为1和9，所以B在发散时，就不会完全把B的信息加载到A上，对称归一化拉普拉斯矩阵，解决了差距大的问题
