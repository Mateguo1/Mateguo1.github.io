---
layout: post
title: U-Net
categories: Neural_Network
description: U-Net
keywords: Neural Network, U-Net
---

# U-Net

U-Net作为一个强大的语义分割模型，<a href="https://arxiv.org/pdf/1505.04597v1.pdf">论文链接</a>，可以通过使用较少数据进行训练即可得到不错的效果，下图是原论文中给出的U-Net的网络结构，概括为encode+decode，下图中主要把它从左到右分为三个部分：encode、decode、output，其中encode与decode之间的连接关系包括直接concat和bottleneck。

![](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/WPS图片编辑.png)

## 1. U-Net：

### 1.1  Encoder和Decoder：

上图中的最左边的框框里，按照图里的顺序命名为encoder1-4、decoder4-1和encoder5吧，其实这个结构可以概括为卷积与最大池化的堆叠，当然了encoder的结构也可以换成VGG、ResNet等网络。

encoder1-4输出的feature map，都会被当作下一层的输入，并且和对应decoder里面的部分进行特征融合（上采样+堆叠），decoder5-2输出的feature map，也都会当作下一层的输入，并于encoder对应的部分进行特征融合，至于卷积核尺寸和特征图的层数图上都表明的很清楚了，这里就不在赘述了，哈哈哈哈。

### 1.2 逆卷积：

这里使用了逆卷积来进行上采样，除逆卷积之外，我还看到好多人用了双线性插值（这个貌似是得改通道数），之后我再把双线性插值补上，这里贴上两个官网给出的链接：<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">逆卷积动图</a>，<a href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf">逆卷积论文</a>，非常值得参考，下面我简单的概述一下。

```python
# pytorch官网源码
#https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d 

torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)
# 这里是pytorch官方给出的代码
torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)

# 具体参数解释
#in_channels (int) – 输入图像中的通道数
#out_channels (int) – 卷积产生的通道数
#kernel_size (int or tuple) – 卷积核的大小
#stride (int or tuple, optional) – 卷积的步幅。默认值：1
#padding (int or tuple, optional) – dilation * (kernel_size - 1) - 零填充将添加到输入中每个维度的两侧。默认值：0
#output_padding (int or tuple, optional) – 添加到输出形状中每个维度一侧的额外尺寸。默认值：0
#groups (int, optional) – 从输入通道到输出通道的阻塞连接数。默认值：1
#bias (bool, optional) – 如果True，则向输出添加可学习的偏差。默认：True
#dilation (int or tuple, optional) – 内核元素之间的间距。默认值：1
```

注意动图蓝色是输入，绿色是输出。

<center class="half">
    <img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif" width="200"/>
    <img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif" width="200"/>
    <img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif" width="200"/>
</center>
直接总结一个泛化的实现思路：（建议对比着动图看）<br/>
（1）对于输入的特征图A进行一些变换，得到新的特征图A‘；<br/>
（2）对新的卷积核进行设置，得到新的卷积核设置；<br/>
（3）用新的卷积核在新的特征图上做常规卷积操作，即可得到逆卷积的结果。

特征图A（H，W），下面用H做举例，W同理，经过变换的A’（H’，W'）中 H‘=H+(stride'-1)×(H-1)。<br/>至于A’是怎么得到的？在输入的A中加入插值（interpolation），来插入0，在原先高度方向每两个相邻的行之间插入stride'-1行的0，这也就说明了如果这时候的stride'设为1，那么size不变。<br/>然后再用A’和新的卷积核，得到最终的结果，首先对于A‘根据卷积的公式，可得H_out = (H'+2×padding-kernel_size)/stride+1，带入上面的公式可得到：H_out = (H+(stride'-1)×(H-1)+2×padding-kernel_size)/stride+1，pytorch官网最终给出的公式中是把stride直接默认为1了。

下面是关于ConvTranspose3d的公式：

$$ D_{out} = (D_{in}-1)×stride[0]-2×padding[0]+dilation[0]×(kernel\_size[0]-1)+output\_padding[0]+1$$

$$ H_{out} = (H_{in}-1)×stride[1]-2×padding[1]+dilation[1]×(kernel\_size[1]-1)+output\_padding[1]+1$$

$$ W_{out} = (W_{in}-1)×stride[2]-2×padding[2]+dilation[2]×(kernel\_size[2]-1)+output\_padding[2]+1$$

ConvTranspose3d最终可学习的参数，包括：ConvTranspose3d.weight$(\text{in\_channels},\frac{out\_channels}{groups},\text{kernel\_size[0],kernel\_size[1],kernel\_size[2]})$和ConvTranspose3d.bias$(out\_channels)$

### 1.3 参考代码：

<a href="https://colab.research.google.com/drive/1mI3CxeVCDW5VRze3IOWrEzcgIcZ9Xepv?usp=sharing">U-Net_Colab</a>

## 2. mIOU：

### 2.1 简述：

均交并比：Mean Intersection over Union

首先来看下IOU，下图黄色部分代表label，蓝色代表prediction，其中相交部分即为TP，有颜色的区域为二者相并，而mIOU就是该数据集中的每一类的交并比的平均。

![image-20211130105004825](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211130105004825.png)

### 2.2 计算

#### （1）混淆矩阵：

行之和为该类的真实样本数量，列之和是预测为该类的样本数量（随便编的一个矩阵）

![](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211130105806196.png)

#### （2）计算mIOU：

针对于每一类IOU：<br/>
交：对角线的值<br/>并：行+列-对角线值
IOU=交/并
mIOU=mean(sum(IOU))

## 3. ResNet+U-Net

用pytorch中的ResNet作为encoder替换U-Net原始结构，可以用ImageNet的预训练权重来fine-tuning，下图是ResNet原论文中的结构。

![image-20200724201909926](https://mateguo1.github.io/assets/img/image-20200724201909926.png)

下图是pytorch官方的ResNet34的源码，然后colab的代码，如下。

![image-20211130143853788](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211130143853788.png)

<a href="https://colab.research.google.com/drive/1r_iV-1im-d74MH4NO-9j5SNzl8Uu3jiY?usp=sharing">ResNet_UNet_Colab</a>
