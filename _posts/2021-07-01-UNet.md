---
layout: post
title: U-Net
categories: Neural_Network
description: U-Net
keywords: Neural Network, U-Net
---

# U-Net

论文题目：U-Net: Convolutional Networks for Biomedical Image Segmentation<br/>论文地址：https://arxiv.org/pdf/1505.04597v1.pdf

作为一个优秀的语义分割模型，可以通过使用较少数据进行训练即可得到不错的效果，下图是原论文中给出的U-Net的网络结构，概括为encode+decode，下图中主要把它从左到右分为三个部分：主干特征提取、加强特征提取、预测。

![](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/WPS%E5%9B%BE%E7%89%87%E7%BC%96%E8%BE%91.png)



## 1. U-Net：

### 1.1  主干特征提取：

上图中的最左边的框框里，按照图里的顺序命名为conv1-conv5吧，其实这个结构可以概括为卷积与最大池化的堆叠，当然了它也可以换成VGG、ResNet等网络。

其中的conv1-conv4输出的feature map，都会被当作下一层的输入，并且和对应decode里面的部分进行特征融合（上采样+堆叠），至于卷积核尺寸和特征图的层数图上都表明的很清楚了，这里不在赘述了。

```python
# encoder
def encode_block(in_channels, out_channels):
    block = torch.nn.Sequential(
        nn.Conv2d(kernel_size=(3, 3), in_channels=in_channels, out_channels=out_channels), #BN层
        nn.ReLU(), 
        nn.BatchNorm2d(out_channels),
        nn.Conv2d(kernel_size=(3, 3), in_channels=out_channels, out_channels=out_channels),
        nn.ReLU(),
        nn.BatchNorm2d(out_channels)
    )
    return block


self.conv_encode_i = encode_block(in_channels=XXX, out_channels=XXX)
self.conv_pool_i = nn.MaxPool2d(kernel_size=2, stride=2)

```

#### 1.1.1 逆卷积：

动图：https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md<br/>论文：https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf

```python
# pytorch官网源码
# https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d

torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)
# 这里是pytorch官方给出的代码
torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)

# 具体参数解释
#in_channels (int) – 输入图像中的通道数
#out_channels (int) – 卷积产生的通道数
#kernel_size (int or tuple) – 卷积核的大小
#stride (int or tuple, optional) – 卷积的步幅。默认值：1
#padding (int or tuple, optional) – dilation * (kernel_size - 1) - 零填充将添加到输入中每个维度的两侧。默认值：0
#output_padding (int or tuple, optional) – 添加到输出形状中每个维度一侧的额外尺寸。默认值：0
#groups (int, optional) – 从输入通道到输出通道的阻塞连接数。默认值：1
#bias (bool, optional) – 如果True，则向输出添加可学习的偏差。默认：True
#dilation (int or tuple, optional) – 内核元素之间的间距。默认值：1
```

注意动图蓝色是输入，绿色是输出。

<center class="half">
    <img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif" width="200"/>
    <img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif" width="200"/>
    <img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif" width="200"/>
</center>
直接总结一个泛化的实现思路：（建议对比着动图看）<br/>
（1）对于输入的特征图A进行一些变换，得到新的特征图A‘；<br/>
（2）对新的卷积核进行设置，得到新的卷积核设置；<br/>
（3）用新的卷积核在新的特征图上做常规卷积操作，即可得到逆卷积的结果。

特征图A（H，W），下面用H做举例，W同理，经过变换的A’（H’，W'）中 H‘=H+(stride'-1)×(H-1)。<br/>至于A’是怎么得到的？在输入的A中加入插值（interpolation），来插入0，在原先高度方向每两个相邻的行之间插入stride'-1行的0，这也就说明了如果这时候的stride'设为1，那么size不变。<br/>然后再用A’和新的卷积核，得到最终的结果，首先对于A‘根据卷积的公式，可得H_out = (H'+2×padding-kernel_size)/stride+1，带入上面的公式可得到：H_out = (H+(stride'-1)×(H-1)+2×padding-kernel_size)/stride+1，pytorch官网最终给出的公式中是把stride直接默认为1了。

下面是关于ConvTranspose3d的公式：

$$ D_{out} = (D_{in}-1)×stride[0]-2×padding[0]+dilation[0]×(kernel\_size[0]-1)+output\_padding[0]+1$$

$$ H_{out} = (H_{in}-1)×stride[1]-2×padding[1]+dilation[1]×(kernel\_size[1]-1)+output\_padding[1]+1$$

$$ W_{out} = (W_{in}-1)×stride[2]-2×padding[2]+dilation[2]×(kernel\_size[2]-1)+output\_padding[2]+1$$

ConvTranspose3d最终可学习的参数，包括：ConvTranspose3d.weight$(\text{in\_channels},\frac{out\_channels}{groups},\text{kernel\_size[0],kernel\_size[1],kernel\_size[2]})$和ConvTranspose3d.bias$(out\_channels)$

#### 1.1.2 膨胀卷积：



### 1.2 加强特征提取：

```python
# decoder
class decode_block(nn.Module):
    def __init__(self, in_channels, mid_channels, out_channels):
        super(expansive_block, self).__init__()
        # 上采样，逆卷积（下面有一个简单的解释）
        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=(3, 3), stride=2, padding=1,
                                     output_padding=1, dilation=1)

        self.block = nn.Sequential(
            nn.Conv2d(kernel_size=(3, 3), in_channels=in_channels, out_channels=mid_channels),
            nn.ReLU(),
            nn.BatchNorm2d(mid_channels),
            nn.Conv2d(kernel_size=(3, 3), in_channels=mid_channels, out_channels=out_channels),
            nn.ReLU(),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, e, d):
        # 上采样
        d = self.up(d)
        # 堆叠
        diffY = e.size()[2] - d.size()[2]
        diffX = e.size()[3] - d.size()[3]
        e = e[:, :, diffY // 2:e.size()[2] - diffY // 2, diffX // 2:e.size()[3] - diffX // 2]
        cat = torch.cat([e, d], dim=1)
        # 卷积
        out = self.block(cat)
        return out
```

### 1.3 预测：

直接就一个1×1的卷积，并且最后调整一下channel数为num_classes 即可

```python
# output
def output_block(in_channels, out_channels):
    block = nn.Sequential(
        nn.Conv2d(kernel_size=(1, 1), in_channels=in_channels, out_channels=out_channels),
        nn.ReLU(),
        nn.BatchNorm2d(out_channels),
    )
    return block
```

### 1.4 bottleneck：

这个在之前的MobileNet中简单提到过，下面是这个结构的代码，然后过段时间我会重新更新一下MobileNet的博文，在其中详细讲一下这个结构：

```python
# Bottleneck
self.bottleneck = torch.nn.Sequential(
	nn.Conv2d(kernel_size=3, in_channels=512, out_channels=1024),
	nn.ReLU(),
	nn.BatchNorm2d(1024),
	nn.Conv2d(kernel_size=3, in_channels=1024, out_channels=1024),
	nn.ReLU(),
	nn.BatchNorm2d(1024)
)
```



## 2. mIOU：

### 2.1 简述：

均交并比：Mean Intersection over Union

首先来看下IOU，下图黄色部分代表label，蓝色代表prediction，其中相交部分即为TP，有颜色的区域为二者相并，而mIOU就是该数据集中的每一类的交并比的平均。

![image-20211130105004825](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211130105004825.png)

### 2.2 计算

#### （1）混淆矩阵：

行之和为该类的真实样本数量，列之和是预测为该类的样本数量（随便编的一个矩阵）

![](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211130105806196.png)

#### （2）计算mIOU：

针对于每一类IOU：<br/>
交：对角线的值<br/>并：行+列-对角线值
IOU=交/并
mIOU=mean(sum(IOU))

## 3. ResNet+U-Net

用pytorch中的ResNet作为encoder替换U-Net原始结构，可以用ImageNet的预训练权重来fine-tuning，下图是ResNet原论文中的结构。

![image-20200724201909926](https://mateguo1.github.io/assets/img/image-20200724201909926.png)

下图是pytorch官方的ResNet34的源码

![image-20211130143853788](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211130143853788.png)

```python
# 根据上图就直接很简单的改写一下encode即可，下面是init中encode的代码
self.resnet = models.resnet34(pretrained=pretrained)
self.layer0 = nn.Sequential(
	self.resnet.conv1,
	self.resnet.bn1,
	self.resnet.relu,
	self.resnet.maxpool
)

# Encode
self.layer1 = self.resnet.layer1
self.layer2 = self.resnet.layer2
self.layer3 = self.resnet.layer3
self.layer4 = self.resnet.layer4
```

