---
layout: post
title: Pytorch
categories: Tutorial
description: Pytorch tutorial
keywords: Tutorial
---

# Pytorch

## 1. 张量：

### 1.1 生成张量：

```python
import torch
import numpy as np
data = np.array([[1,2,3],[4,5,6],[7,8,9]])
# 1. 类构造函数，根据全局缺省值
T1 = torch.Tensor(data)
# 2. 工厂函数，根据输入
T2 = torch.tensor(data)
T3 = torch.as_tensor(data)# 输入变，T3映射，亦变
T4 = torch.from_numpy(data)# 输入变，T4映射，亦变
'''
T1、T2复制存于新内存
T3、T4是在内存中共享
最好是tensor和as_tensor
'''
# 3.无中生有
T5 = torch.eye(n)# 单位张量
T6 = torch.zeros(a,b,...,n)# 零张量
T7 = torch.ones(a,b,...,n)# 一张量
T8 = torch.rand(a,b,...,n)# 随机张量[0,1]

```

工厂函数：[Factory (object-oriented programming) - Wikipedia](https://en.wikipedia.org/wiki/Factory_(object-oriented_programming))

构造函数：[Constructor (object-oriented programming) - Wikipedia](https://en.wikipedia.org/wiki/Constructor_(object-oriented_programming))

### 1.2 张量属性：

#### 1.2.1 张量的秩：

rank

表示张量的维数，等于张量形状的数组的数量

```python
len(T.shape)
```

#### 1.2.2 张量的轴：

axis

等于张量形状数组的最后一个数的数值

#### 1.2.3 张量的形状：

```python
T.shape
```

#### 1.2.4 张量的数据类型：

张量间的数据运算必须发生在相同类型的张量之间

```python
T.dtype
```

#### 1.2.5 张量的设备：

cpu or gpu

张量间的数据运算必须发生在同一设备的张量之间

```python
T.device
device = torch.device("cuda:0")
```

#### 1.2.6 张量的布局：

```python
T.layout
```

### 1.3 张量操作

#### 1.3.1 重塑形：

```python
T.reshape(n,m,-1)
#张量的标量分量，也就是元素数量
torch.tensor(t.shape).prod()
T.numel()
```

reshape里数值的乘积应该与元素总数相等，-1可用于替换一个数，但是其余数的乘积需要被元素总数除尽。

##### 1.3.1.1 压缩、解缩：

```python
#用于改变张量的维度
T.reshape(1,12).squeeze()#移除所有长度为一的轴
T.reshape(1,12).squeeze().unsqueeze(dim=0)#增加一个长度为一的维度
```

##### 1.3.1.2 flatten：

```python
def flatten(T):
    T = T.reshape(1,-1)
    T = T.sequeeze()
    return T
#pytorch自带flatten
```

#### 1.3.2 拼接：

#### 1.3.2.1 cat：

```python
T1 = torch.tensor([[1,2],[3,4]])
T2 = torch.tensor([[5,6],[7,8]])
#行拼接，列数不变
torch.cat((T1,T2),dim=0)
#列拼接，行数不变
torch.cat((T1,T2),dim=1)
```

#### 1.3.2.2 stack：

常用于批处理

#### 1.4 批处理：

##### 1.4.1 CNN的图像输入的形状：

[Batchsize, Color channel, Height, Width]

##### 1.4.2 flatten举例：

![1621412690943](C:\Users\Mate\AppData\Roaming\Typora\typora-user-images\1621412690943.png)

```python
#因此对于每一张图片的flatten就要单独来做了
T.flatten(start_dim=1)
```



#### torch.ones_like():

```python
input = torch.empty(1,2)
torch.ones_like(input) # 按照input的形状，填充出来一个值全为一的矩阵
```

## call, init, forward:

init：类的初始化函数<br/>call：使类具有类似函数的功能

```python
#通过这个例子来看看三者的一个功能吧
class A():
    # 初始化一个年龄
    def __init__(self, init_age):
        super().__init__()
        print('2. 它现在年龄是:',init_age)
        self.age = init_age
    # 启动函数
    def __call__(self, added_age):
        print('3. 准备调用forward函数了')
        res = self.forward(added_age)
        return res
    # forward，年龄增长
    def forward(self, input_):
        print('4. forward函数被调用，年龄增长')
        return input_ + self.age
    
print('1. 创建对象')
#创建对象，给一个初始值
a = A(10) 
#这里是传参给了call，它再调用了forward
input_param = a(2) 
print("5. 它增长后的年龄是：", input_param)

```

```python
out：
1. 创建对象
2. 它现在年龄是: 10
3. 准备调用forward函数了
4. forward函数被调用，年龄增长
5. 它增长后的年龄是： 12
```

而pytorch在nn.Module中，实现了call方法，并且在call方法中调用了forward函数，也就是说在用的时候，首先创建类对象object，然后使用object(input)，这里传参给了并且调用了call，然后call调用了forward，因此在创建多层神经网络模块时，只需要实现init和forward就成。

![image-20211129213651942](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/image-20211129213651942.png)
