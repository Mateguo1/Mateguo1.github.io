---
layout: post
title: Transformer
description: deep-learning
keywords: deep-learning
---

# Transformer:

今天新开一个坑，主要是为了ViT（虽然之前好多坑都没填完，哈哈哈哈），首先《Attention Is All You Need》论文链接：https://arxiv.org/pdf/1706.03762.pdf。

接下来，先整理下李宏毅老师的ML课上讲的transformer，课件链接：https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/seq2seq_v9.pdf，然后这部分主要是针对我听完课程之后的一些个人的思路总结，顺序上可能和课件不太一样，因此还是很建议大家直接去看李老师的ML课程视频的，讲的可太好了！！！

按惯例，直接来吧，先上网络结构，下图是原论文中给出的

![Transformer_0](https://raw.githubusercontent.com/Mateguo1/Pictures/master/img/Transformer_0.png)
