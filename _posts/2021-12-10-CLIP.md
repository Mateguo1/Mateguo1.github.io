---
layout: post
title: CLIP
description: deep-learning
categories: Neural_Network
keywords: deep-learning
---

# CLIP:

太牛了，这篇博客只是简单的整理下这篇论文的内容，论文链接：<a href="https://proceedings.mlr.press/v139/radford21a/radford21a.pdf">Learning Transferable Visual Models From Natural Language Supervision</a>。

目前CV领域的SOTA方法一般是基于预定的物体标签进行训练的，这也就限制了其泛化能力。因此，该论文提出了一种方法，在经过预训练之后，根据自然语言，继续学习新的视觉概念，从而使在zero-shot情况下，将模型迁移到下游任务，并取得较好的效果成为可能。

首先来看文中给出的结构图，下面就按照图中的顺序来解释下该方法的基本逻辑。

![image-20220616161150469](/Users/guoziyu/Library/Application Support/typora-user-images/image-20220616161150469.png)

## 1. Contrastive Pre-training：

首先是使用多模态、对比学习的方法来训练模型，Image Encoder和Text Encoder的输入分别是一系列的图像和自然语言，然后再将通过各自的Encoder得到的特征向量做个类似于混淆矩阵的矩阵。（这里应该对比学习的方法，我还没了解特别多，之后会出一篇博文来整理下这部分的知识）

矩阵对角线上蓝色部分，意味着Image和Text是同对应的（正类），而其他白色部分则不是对应的（负类），然后算loss等等。

## 2. Create Dataset Classifier：

根据文本创建新的数据分类项，这里涉及到论文后面讲到的Promt Engineering and Ensembling这部分。简单说下，因为单词的多义性以及防止出现过大的分布偏移，就用了模版，像是图中的 A photo of a {object}，对于能够特定化描述图像文本很有帮助。

## 3. Zero-shot Prediction：

然后再将要预测的图片和上述所有的分类项，创建矩阵，进行对比，就能得到相应的类别了。

文中还给出了CLIP核心算法的伪代码，我贴到下面了，可以帮助理解。

```python
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# t - learned temperature parameter

# extract feature representations of each modality
I_f = image_encoder(I) #[n, d_i]
T_f = text_encoder(T) #[n, d_t]

# joint multimodal embedding [n, d_e]
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)

# scaled pairwise cosine similarities [n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)

# symmetric loss function
labels = np.arange(n)
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t)/2
```

